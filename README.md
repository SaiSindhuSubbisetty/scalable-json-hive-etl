
---

### ðŸ”· Scalable JSON Hive ETL using PySpark

This project demonstrates a simple and scalable **ETL (Extract, Transform, Load)** pipeline built with **Apache Spark 4.0.0** using **PySpark**. It reads **JSON files**, processes the data using Spark DataFrames, and writes the output into **Parquet** format, suitable for further analysis or loading into **Apache Hive**.

---

### ðŸ“Œ Features

* ðŸ”¹ Reads JSON data using PySpark
* ðŸ”¹ Parses and displays structured data
* ðŸ”¹ Writes output in optimized **Parquet** format
* ðŸ”¹ Prepares data for Hive ingestion
* ðŸ”¹ Built with **Spark 4.0.0**, compatible with Hadoop ecosystem

---

### ðŸ“‚ Folder Structure

```
scalable-json-hive-etl/
â”‚
â”œâ”€â”€ data/                      # Input JSON data
â”‚   â””â”€â”€ sample.json
â”‚
â”œâ”€â”€ etl/                       # ETL Python scripts
â”‚   â””â”€â”€ spark_etl.py
â”‚
â”œâ”€â”€ warehouse/                # Output Parquet files (auto-created)
â”‚
â”œâ”€â”€ README.md                 # Project description
â””â”€â”€ requirements.txt          # Python dependencies
```

---

### ðŸš€ Run Locally

1. Set up virtual environment and install dependencies:

   ```bash
   python -m venv .venv
   .venv\Scripts\activate
   pip install pyspark
   ```

2. Run the ETL script:

   ```bash
   python etl/spark_etl.py
   ```

3. Output will be stored in `warehouse/output_parquet/`.

---

### ðŸ“„ Sample Input (`data/sample.json`)

```json
[
  {"id": 1, "city": "Bangalore"},
  {"id": 2, "city": "Hyderabad"}
]
```

